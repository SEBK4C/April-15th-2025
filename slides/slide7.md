<!-- .slide: data-background="linear-gradient(to bottom right, #004477, #007799)" -->

## LLMs Go Local: Run AI Anywhere!

The friction is disappearing...

- **Enter Llamafile:** Packages LLM + Runtime into a single executable.
- **Truly Portable:** Runs on macOS, Windows, Linux, *BSD... often with zero setup!
- **Verifiable Integrity:** Single file allows hash verification (know what you're running).
- **Certified Expertise:** Potential for pre-tested models expert in specific domains (health, code, math).
- **Democratizing AI:** Powerful, trustworthy models directly on your machine, offline.

<!-- .element: class="fragment" -->
> What could *you* build if trustworthy AI ran anywhere, friction-free?

Note: Shift from deployment friction to the emerging ease, security, and verifiable expertise of local, portable AI like Llamafile. 